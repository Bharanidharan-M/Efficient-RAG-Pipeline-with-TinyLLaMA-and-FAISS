# ðŸ“˜ Retrieval-Augmented QA with TinyLLaMA and LangChain

This project demonstrates a lightweight Retrieval-Augmented Generation (RAG) pipeline using:

- ðŸ¦™ **TinyLLaMA** for local LLM inference (via `llama-cpp-python`)
- ðŸ”— **LangChain** for orchestration and chaining
- ðŸ“š **FAISS** for fast vector-based document retrieval
- ðŸ§  **HuggingFace Embeddings** (`all-MiniLM-L6-v2`)
- ðŸ“„ Custom documents for context-aware Question Answering

---

## ðŸš€ Features

- âœ… Runs fully **offline** with no OpenAI API key needed
- âœ… Loads and indexes text documents from local disk
- âœ… Performs chunked text splitting and dense embedding
- âœ… Supports **natural language Q&A** over documents using a local LLM

---

## ðŸ§± Architecture

```plaintext
[Your Documents] â†’ [Text Splitter] â†’ [Embeddings] â†’ [FAISS Index]
                                   â†“
                             [LangChain Retriever]
                                   â†“
                               [TinyLLaMA (LLM)]
                                   â†“
                              [Final Answer]
